{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6de4ea-25fa-48ec-87f2-cebbca9d308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\anaconda\\lib\\site-packages (from librosa) (0.59.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\anaconda\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\anaconda\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\anaconda\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\anaconda\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-0.5.0.post1-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\anaconda\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\anaconda\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\anaconda\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from lazy_loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\anaconda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\anaconda\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\anaconda\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Using cached soxr-0.5.0.post1-cp312-abi3-win_amd64.whl (164 kB)\n",
      "Installing collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ea5336-982c-4809-8e1a-ea3aafb8902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': {'input_ids': tensor([[ 101, 2821, 2026, 2643, 1010, 2002, 2015, 2439, 2009, 1012, 2002, 2015,\n",
      "         6135, 2439, 2009, 1012,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}, 'audio': tensor([[[ 0.6532,  0.9250,  0.7307,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1452,  1.5039,  1.3053,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.4017,  1.5355,  1.4726,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3351, -1.0039, -1.4250,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4579, -1.1415, -1.5299,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.7148, -1.5772, -1.9008,  ...,  0.0000,  0.0000,  0.0000]]]), 'emotion': tensor(5), 'sentiment': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import cv2\n",
    "import torch \n",
    "import numpy as np\n",
    "import subprocess\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, csv_path, video_dir):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.video_dir = video_dir\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.emotion_map = {\n",
    "            'anger': 0,\n",
    "            'disgust': 1,\n",
    "            'fear': 2,\n",
    "            'joy': 3,\n",
    "            'neutral': 4,\n",
    "            'sadness': 5,\n",
    "            'surprise': 6,\n",
    "        }\n",
    "\n",
    "        self.sentiment_map = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 2,\n",
    "        }\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        try:\n",
    "            if not cap.isOpened():\n",
    "                raise FileNotFoundError(f\"File {video_path} not found\")\n",
    "            \n",
    "            ret, frame= cap.read()\n",
    "            if not ret or frame is None:    \n",
    "                raise ValueError(f\"Cannot read frame from {video_path}\")\n",
    "            \n",
    "            #reset the video capture to the beginning\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            \n",
    "            while len(frames) < 30 and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or frame is None:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frame = frame/255.0\n",
    "                frames.append(frame)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading video {video_path}: {e}\")\n",
    "        finally:    \n",
    "            cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"Video {video_path} has no frames\")\n",
    "        \n",
    "        #pad or truncate frames\n",
    "        if len(frames) < 30:\n",
    "            frames += [np.zeros_like(frames[0])] * (30 - len(frames))\n",
    "        else:    \n",
    "            frames = frames[:30]\n",
    "\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0,3,1,2)\n",
    "    \n",
    "    def extract_audio_features(self, video_path):\n",
    "        audio_path = video_path.replace('.mp4', '.wav')\n",
    "        try:\n",
    "            # Convert video to audio using ffmpeg\n",
    "            subprocess.run([\n",
    "                'ffmpeg',\n",
    "                '-i', video_path,\n",
    "                '-vn',\n",
    "                '-acodec', 'pcm_s16le',\n",
    "                '-ar', '16000',\n",
    "                '-ac', '1',\n",
    "                audio_path\n",
    "                ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    " \n",
    "            # Load audio with librosa\n",
    "            y, sr = librosa.load(audio_path, sr=16000)\n",
    "            \n",
    "            # Compute mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_fft=1024, \n",
    "                hop_length=512, \n",
    "                n_mels=64\n",
    "            )\n",
    "            \n",
    "            # Convert to log scale (dB)\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "            \n",
    "            # Normalize the spectrogram\n",
    "            log_mel_spec = (log_mel_spec - np.mean(log_mel_spec)) / (np.std(log_mel_spec) + 1e-8)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            mel_tensor = torch.FloatTensor(log_mel_spec)\n",
    "            \n",
    "            # Handle padding/truncation to fixed size (300 frames)\n",
    "            if mel_tensor.size(1) < 300:\n",
    "                padding = 300 - mel_tensor.size(1)\n",
    "                mel_tensor = torch.nn.functional.pad(mel_tensor, (0, padding))\n",
    "            else:\n",
    "                mel_tensor = mel_tensor[:, :300]\n",
    "            \n",
    "            # Add batch dimension to match torchaudio output\n",
    "            mel_tensor = mel_tensor.unsqueeze(0)\n",
    "            \n",
    "            return mel_tensor\n",
    "        \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise ValueError(f\"Error extracting audio from video: {e}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error converting video to audio: {e}\")\n",
    "        finally:\n",
    "            if os.path.exists(audio_path):\n",
    "                os.remove(audio_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        video_filename = f\"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}.mp4\"\n",
    "        video_path = os.path.join(self.video_dir, video_filename)\n",
    "\n",
    "\n",
    "        video_path_exists = os.path.exists(video_path)\n",
    "\n",
    "        if video_path_exists == False:\n",
    "            raise FileNotFoundError(f\"File {video_path} not found\")\n",
    "        \n",
    "        text_inputs = self.tokenizer(row['Utterance'],\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=True,\n",
    "                                    max_length=128, \n",
    "                                    return_tensors='pt')\n",
    "        \n",
    "        # video_frames = self.load_video_frames(video_path)\n",
    "        audio_features = self.extract_audio_features(video_path)\n",
    "        \n",
    "        emotion_label = self.emotion_map[row['Emotion']]\n",
    "        sentiment_label = self.sentiment_map[row['Sentiment']]\n",
    "        \n",
    "        return {\n",
    "            'text': text_inputs,\n",
    "            'audio': audio_features,\n",
    "            'emotion': torch.tensor(emotion_label),\n",
    "            'sentiment': torch.tensor(sentiment_label)\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    meld = MELDDataset(r'../dataset/dev/dev_sent_emo.csv',r'../dataset/dev/dev_splits_complete')\n",
    "\n",
    "    print(meld[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9d61ec-ac01-4aef-ac24-ea18dc217076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8887,  0.0593,  0.1778,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7168,  0.3129, -0.2175,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8788,  0.0870, -0.2390,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.5908, -1.1228, -0.6397,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6323, -1.1792, -1.1183,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6729, -1.0245, -0.9799,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.7215, -1.1688, -1.2858,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2546,  0.0808,  0.1007,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2361,  0.8323,  0.6341,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.5239, -1.5591, -1.4921,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.8124, -1.7578, -1.4530,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.0530, -1.9213, -2.2435,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.3945,  0.7483,  0.7822,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5303,  1.0367,  1.1040,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6194,  0.6291,  0.1760,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3679, -1.2956, -1.3959,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4447, -1.4167, -1.5287,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.5166, -1.8821, -1.9437,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.6643,  1.3216,  0.9539,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7110,  1.2500,  0.5790,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7923,  0.8570,  1.2869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.0209, -0.7244, -0.8118,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3899, -0.7922, -0.4386,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9808, -0.2682, -0.6745,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.2102,  0.3686,  0.4305,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2592,  0.6997,  0.8263,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6071,  0.3638,  0.9014,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.5436, -1.7522, -1.9404,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.5138, -1.8206, -2.2221,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.5040, -2.0755, -2.2800,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.4780,  0.9024,  0.6961,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4310,  1.0554,  0.6831,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5104,  0.9818,  0.4751,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.8718, -1.4311, -1.5563,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3849, -1.7858, -1.6348,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.2542, -2.0727, -2.1245,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.7050,  0.4474,  0.9286,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7861,  0.5206,  1.3279,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8631,  0.8984,  1.3188,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.5330, -0.1026, -0.8175,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6951, -0.5645, -1.1545,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8908, -0.8798, -1.0945,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.0510,  0.7989,  0.9565,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7022,  1.4476,  0.9732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5850,  1.1369,  0.4482,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 2.3493,  1.1988, -0.4934,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 2.1020,  1.0735, -0.7078,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.6295,  0.6593, -1.1443,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-9.0972e-02,  5.0507e-02,  2.1742e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4705e-01,  1.0081e-01,  3.4229e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.9333e-01,  6.6966e-02,  6.6732e-04,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-5.5057e-01, -1.5757e+00, -1.8233e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-6.3280e-01, -1.2646e+00, -1.7421e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-5.9017e-01, -1.7086e+00, -1.7895e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]])\n",
      "tensor([[[-0.5283, -0.8663, -0.2880,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0915,  0.0395,  0.3932,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6156,  0.6745,  0.7005,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.6314, -1.4989, -1.6478,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.5191, -1.3786, -1.6119,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.8692, -1.6419, -1.9649,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.3670,  0.1478,  0.2801,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7082,  0.2874,  0.1293,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.4559,  0.7675,  0.1383,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.5413, -0.9469, -0.8535,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5470, -1.0518, -1.0146,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9114, -1.3058, -1.2289,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.8373, -0.2410, -0.4049,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3985,  0.0023, -0.3206,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5130, -0.1890, -0.4945,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0431, -0.4287,  0.2106,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2121, -0.1427, -0.4407,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2786, -0.4295, -0.8691,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.4528,  0.0830,  0.5496,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1048,  0.0973,  0.6104,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4410,  0.7376,  0.6972,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.6400, -1.3271, -1.1608,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.6763, -1.3107, -1.2879,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.7597, -1.5827, -1.5389,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-5.4250e-01,  2.1473e-01,  2.3596e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-3.8557e-01,  1.7030e-01,  1.6182e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.6332e-03,  2.1970e-01,  4.8142e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-1.1429e+00, -1.1684e+00, -8.5365e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.4886e+00, -1.4668e+00, -1.3293e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.8214e+00, -1.8845e+00, -1.5248e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]])\n",
      "tensor([[[-0.6729,  0.1326,  0.2875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4483,  0.4674,  0.1873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.5369,  0.4213, -0.0564,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.6582, -0.4494,  0.2796,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8509, -0.4096,  0.0538,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1559, -0.6273, -0.5295,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.2458,  0.1798,  0.3001,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4742,  1.8527,  1.2427,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9163,  2.4553,  2.2915,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3151, -1.4794, -1.8918,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0939, -1.7082, -2.1175,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3625, -1.5023, -2.2040,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 1.3255,  0.9814,  0.6634,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.4606,  1.1170,  1.1017,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 2.1190,  2.3306,  2.3109,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.4499, -0.5681, -0.3722,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6578, -0.8733, -0.9570,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.6275, -1.1366, -1.1122,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.5959, -0.5871, -0.4606,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0017,  0.4291,  0.2900,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7848,  1.2090,  0.5023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.0938, -0.3397, -1.1158,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7809, -0.5569, -0.8195,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4594, -1.1067, -1.0635,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.4966,  0.8672,  1.0797,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6858,  1.0197,  1.1393,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1270,  0.3182,  0.7503,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.7902, -1.5149, -1.6251,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.8810, -1.7543, -1.7516,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.2852, -2.0522, -2.0665,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.0219, -0.0055,  0.0172,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0355,  0.3755,  0.1744,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1085,  0.1766,  0.1533,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.5930, -1.3625, -1.4414,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.7183, -1.5050, -1.4386,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.9421, -1.7359, -1.6035,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.7413,  0.7131,  1.4168,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5372,  1.0835,  1.1447,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3323,  0.5624,  0.7566,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.5310, -0.9288, -1.0985,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3191, -1.0099, -0.6076,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0029, -1.1863, -0.9692,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.4457,  0.9371,  1.0375,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5622,  0.7757,  0.8248,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1231,  0.3139,  0.2134,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.8965, -1.5915, -1.5843,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.0289, -1.6256, -1.5940,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3287, -1.8981, -1.7984,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.2592,  0.4870,  0.7204,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3676,  0.7117,  0.4544,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0942,  0.7979,  0.8988,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-2.3515, -2.1381, -2.1083,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.4922, -2.1923, -2.3418,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.9686, -2.3322, -2.5759,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.0248,  0.3928,  0.3101,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1095,  0.5490,  0.7599,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1662,  0.7072,  0.5169,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.5238, -1.2745, -1.4993,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.7841, -1.6482, -1.8134,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3369, -2.0603, -2.2856,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 1.3325,  1.7971,  1.6350,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.2326,  1.5697,  1.5395,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1191,  1.1610,  1.1098,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.7302, -1.0031, -1.3671,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.6920, -1.2221, -1.0491,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3670, -1.7137, -1.7778,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.5088, -0.6755, -1.0580,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3741, -0.0176, -0.3375,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0753,  0.3260, -0.0211,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.0261, -1.4424, -1.0714,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0046, -1.7891, -1.1796,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0783, -1.9978, -1.6225,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.3470,  0.2776,  0.7802,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3756,  0.6492,  0.6136,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3723,  0.6657,  0.8427,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.6270, -1.3815, -1.7503,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.9451, -1.5632, -2.0235,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.0574, -2.1386, -2.3575,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.2614,  0.3370,  0.4054,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1129,  0.0774,  0.2451,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3688, -0.0293, -0.0703,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5359, -0.6237, -1.5940,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5979, -0.9516, -1.6083,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0974, -1.0767, -1.7702,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.2928,  0.7713,  1.0541,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8370,  0.7119,  1.0087,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.0463,  0.8571,  1.0261,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.3461, -1.2287, -1.1066,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9774, -1.1120, -1.0239,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1215, -1.1882, -1.0186,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.5042, -0.8216, -0.5813,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2198,  0.4368,  0.2024,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1630,  0.4419,  0.3105,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.4341, -1.5523, -1.7040,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.8182, -1.5432, -1.7199,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.9858, -1.8681, -2.0193,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[ 0.5646,  0.3122,  0.3076,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1099,  1.1615,  0.5992,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.2482,  1.5885,  1.3468,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-1.2856, -1.0577, -1.1382,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1001, -0.6304, -1.1810,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.3263, -0.9106, -1.4038,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[-0.2911,  0.5411,  0.2881,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1108,  0.5370,  0.2330,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3122,  0.3827,  0.3993,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-2.2271, -1.3410, -1.1554,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.1580, -1.7162, -1.4503,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.1554, -1.8732, -1.4343,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "{'input_ids': tensor([[  101,  2821,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2064,  1045,  ...,     0,     0,     0],\n",
      "        [  101,  2053,  1010,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1045,  2113,  ...,     0,     0,     0],\n",
      "        [  101,  2017,  6655,  ...,     0,     0,     0],\n",
      "        [  101, 10047,  3374,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "torch.Size([32, 30, 3, 224, 224])\n",
      "torch.Size([32, 1, 64, 300])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import cv2\n",
    "import torch \n",
    "import numpy as np\n",
    "import subprocess\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, csv_path, video_dir):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.video_dir = video_dir\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.emotion_map = {\n",
    "            'anger': 0,\n",
    "            'disgust': 1,\n",
    "            'fear': 2,\n",
    "            'joy': 3,\n",
    "            'neutral': 4,\n",
    "            'sadness': 5,\n",
    "            'surprise': 6,\n",
    "        }\n",
    "\n",
    "        self.sentiment_map = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 2,\n",
    "        }\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        try:\n",
    "            if not cap.isOpened():\n",
    "                raise FileNotFoundError(f\"File {video_path} not found\")\n",
    "            \n",
    "            ret, frame= cap.read()\n",
    "            if not ret or frame is None:    \n",
    "                raise ValueError(f\"Cannot read frame from {video_path}\")\n",
    "            \n",
    "            #reset the video capture to the beginning\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            \n",
    "            while len(frames) < 30 and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or frame is None:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frame = frame/255.0\n",
    "                frames.append(frame)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading video {video_path}: {e}\")\n",
    "        finally:    \n",
    "            cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"Video {video_path} has no frames\")\n",
    "        \n",
    "        #pad or truncate frames\n",
    "        if len(frames) < 30:\n",
    "            padding = [np.zeros_like(frames[0]) for _ in range(30 - len(frames))]\n",
    "            frames.extend(padding)\n",
    "        else:    \n",
    "            frames = frames[:30]\n",
    "\n",
    "        #before permute the shape is (30, 224, 224, 3){30 frames, 224x224 resolution, 3 channels}\n",
    "        #after permute the shape is (30, 3, 224, 224){30 frames, 3 channels, 224x224 resolution}\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0,3,1,2)\n",
    "    \n",
    "    def extract_audio_features(self, video_path):\n",
    "        audio_path = video_path.replace('.mp4', '.wav')\n",
    "        try:\n",
    "            # Convert video to audio using ffmpeg\n",
    "            subprocess.run([\n",
    "                'ffmpeg',\n",
    "                '-i', video_path,\n",
    "                '-vn',\n",
    "                '-acodec', 'pcm_s16le',\n",
    "                '-ar', '16000',\n",
    "                '-ac', '1',\n",
    "                audio_path\n",
    "                ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    " \n",
    "            # Load audio with librosa\n",
    "            y, sr = librosa.load(audio_path, sr=16000)\n",
    "            \n",
    "            # Compute mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_fft=1024, \n",
    "                hop_length=512, \n",
    "                n_mels=64\n",
    "            )\n",
    "            \n",
    "            # Convert to log scale (dB)\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "            \n",
    "            # Normalize the spectrogram\n",
    "            log_mel_spec = (log_mel_spec - np.mean(log_mel_spec)) / (np.std(log_mel_spec) + 1e-8)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            mel_tensor = torch.FloatTensor(log_mel_spec)\n",
    "            \n",
    "            # Handle padding/truncation to fixed size (300 frames)\n",
    "            if mel_tensor.size(1) < 300:\n",
    "                padding = 300 - mel_tensor.size(1)\n",
    "                mel_tensor = torch.nn.functional.pad(mel_tensor, (0, padding))\n",
    "            else:\n",
    "                mel_tensor = mel_tensor[:, :300]\n",
    "            \n",
    "            # Add batch dimension to match torchaudio output\n",
    "            mel_tensor = mel_tensor.unsqueeze(0)\n",
    "            \n",
    "            return mel_tensor\n",
    "        \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise ValueError(f\"Error extracting audio from video: {e}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error converting video to audio: {e}\")\n",
    "        finally:\n",
    "            if os.path.exists(audio_path):\n",
    "                os.remove(audio_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        try:\n",
    "            video_filename = f\"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}.mp4\"\n",
    "            video_path = os.path.join(self.video_dir, video_filename)\n",
    "\n",
    "            video_path_exists = os.path.exists(video_path)\n",
    "\n",
    "            if video_path_exists == False:\n",
    "                raise FileNotFoundError(f\"File {video_path} not found\")\n",
    "            \n",
    "            text_inputs = self.tokenizer(row['Utterance'],\n",
    "                                        padding=\"max_length\",\n",
    "                                        truncation=True,\n",
    "                                        max_length=128, \n",
    "                                        return_tensors='pt')\n",
    "            \n",
    "            video_frames = self.load_video_frames(video_path)\n",
    "            audio_features = self.extract_audio_features(video_path)\n",
    "            print(audio_features)\n",
    "\n",
    "            #map sentiment and emotion labels\n",
    "            emotion_label = self.emotion_map[row['Emotion'].lower()]\n",
    "            sentiment_label = self.sentiment_map[row['Sentiment'].lower()]\n",
    "\n",
    "            return {\n",
    "                'text_inputs':{\n",
    "                    'input_ids': text_inputs['input_ids'].squeeze(),\n",
    "                    'attention_mask': text_inputs['attention_mask'].squeeze()\n",
    "                },\n",
    "                'video_frames': video_frames,\n",
    "                'audio_features': audio_features,\n",
    "                'emotion_label': torch.tensor(emotion_label),\n",
    "                'sentiment_label': torch.tensor(sentiment_label)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {video_path}:{e}\")\n",
    "            return None\n",
    "        \n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def prepare_dataloader(train_csv, train_video_dir, dev_csv, dev_video_dir, test_csv, test_video_dir, batch_size=32):\n",
    "    train_dataset = MELDDataset(train_csv, train_video_dir)\n",
    "    dev_dataset = MELDDataset(dev_csv, dev_video_dir)\n",
    "    test_dataset = MELDDataset(test_csv, test_video_dir)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, dev_loader, test_loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, dev_loader, test_loader = prepare_dataloader(\n",
    "        '../dataset/train/train_sent_emo.csv', '../dataset/train/train_splits',\n",
    "        '../dataset/dev/dev_sent_emo.csv', '../dataset/dev/dev_splits_complete',\n",
    "        '../dataset/test/test_sent_emo.csv', '../dataset/test/output_repeated_splits_test'\n",
    "    )\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        print(batch['text_inputs'])\n",
    "        print(batch['video_frames'].shape)\n",
    "        print(batch['audio_features'].shape)\n",
    "        print(batch['emotion_label'].shape)\n",
    "        print(batch['sentiment_label'].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1cd24-bcbc-46e3-a7d0-c551cadd5e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
